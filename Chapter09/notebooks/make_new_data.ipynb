{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta, timezone\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import boto3\n",
    "import io\n",
    "import requests\n",
    "import tempfile\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "from time import sleep, gmtime, strftime\n",
    "from threading import Thread\n",
    "\n",
    "from sagemaker import get_execution_role, session, Session, image_uris\n",
    "from sagemaker.s3 import S3Downloader, S3Uploader\n",
    "from sagemaker.processing import ProcessingJob\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.model_monitor import DataCaptureConfig\n",
    "\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "boto_session = boto3.Session(region_name=region)\n",
    "sagemaker_client = boto_session.client(service_name='sagemaker', region_name=region)\n",
    "featurestore_runtime = boto_session.client(service_name='sagemaker-featurestore-runtime', region_name=region)\n",
    "session = Session()\n",
    "feature_store_session = Session(\n",
    "boto_session=boto_session,\n",
    "sagemaker_client=sagemaker_client,\n",
    "sagemaker_featurestore_runtime_client=featurestore_runtime\n",
    ")\n",
    "s3 = boto3.client('s3')\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Execution role\n",
    "role = get_execution_role()\n",
    "print(\"RoleArn:\", role)\n",
    "\n",
    "region = session.boto_region_name\n",
    "print(\"Region:\", region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline Data bucket\n",
    "data_bucket = 'data-us-east-2-500842391574'\n",
    "raw_key = 'input/raw/abalone.csv'\n",
    "print(\"Raw Data bucket:\", data_bucket)\n",
    "\n",
    "# Setup S3 bucket parmaters for the production logs bucket\n",
    "# Enter the name of the Production Logs Bucket, created by the MLOps Pipeline\n",
    "bucket = 'proddeploymentstage-prodappl-logss3bucket004b0f70-17h5c8ln5qm5a'\n",
    "print(\"Production Logs Bucket:\", bucket)\n",
    "\n",
    "# S3 prefixes\n",
    "data_capture_prefix = 'endpoint-data-capture'\n",
    "s3_capture_upload_path = f's3://{bucket}/{data_capture_prefix}'\n",
    "ground_truth_upload_path = f's3://{bucket}/ground-truth-data/{datetime.now():%Y-%m-%d-%H-%M-%S}'\n",
    "\n",
    "# Get the model monitor image\n",
    "monitor_image_uri = image_uris.retrieve(framework=\"model-monitor\", region=region)\n",
    "\n",
    "print(\"Image URI:\", monitor_image_uri)\n",
    "print(f\"Capture path: {s3_capture_upload_path}\")\n",
    "print(f\"Ground truth path: {ground_truth_upload_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__GET RAW DATA__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'raw' data column names\n",
    "names = [\n",
    "    'sex',\n",
    "    'length',\n",
    "    'diameter',\n",
    "    'height',\n",
    "    'whole_weight',\n",
    "    'shucked_weight',\n",
    "    'viscera_weight',\n",
    "    'shell_weight',\n",
    "    'rings'\n",
    "]\n",
    "\n",
    "# Location of the 'raw' data\n",
    "obj = s3.get_object(Bucket=data_bucket, Key=raw_key)\n",
    "raw_df = pd.read_csv(io.BytesIO(obj['Body'].read()), encoding='utf8', names=names)\n",
    "raw_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "__GET FAKE DATA__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_df = pd.read_csv(\n",
    "    'fake-abalone.csv',\n",
    "    names=names,\n",
    ")\n",
    "fake_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.concat([raw_df, fake_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_csv('../data/abalone.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.m5.large",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
