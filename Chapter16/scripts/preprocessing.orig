import argparse
import os
import requests
import tempfile
import numpy as np
import pandas as pd
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder


# Since we get a headerless CSV file we specify the column names here.
feature_columns_names = [
    'sex',
    'length',
    'diameter',
    'height',
    'whole_weight',
    'shucked_weight',
    'viscera_weight',
    'shell_weight',
]
label_column = 'rings'

feature_columns_dtype = {
    'sex': str,
    'length': np.float64,
    'diameter': np.float64,
    'height': np.float64,
    'whole_weight': np.float64,
    'shucked_weight': np.float64,
    'viscera_weight': np.float64,
    'shell_weight': np.float64
}
label_column_dtype = {'rings': np.float64}

def merge_two_dicts(x, y):
    z = x.copy()
    z.update(y)
    return z


if __name__ == '__main__':
    base_dir = '/opt/ml/processing'

    df = pd.read_csv(
        f'{base_dir}/input/data/abalone.csv',
        header=None, 
        names=feature_columns_names + [label_column],
        dtype=merge_two_dicts(feature_columns_dtype, label_column_dtype)
    )
    numeric_features = list(feature_columns_names)
    numeric_features.remove('sex')
    numeric_transformer = Pipeline(
        steps=[
            ('imputer', SimpleImputer(strategy='median')),
            ('scaler', StandardScaler())
        ]
    )

    categorical_features = ['sex']
    categorical_transformer = Pipeline(
        steps=[
            ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
            ('onehot', OneHotEncoder(handle_unknown='ignore'))
        ]
    )

    preprocess = ColumnTransformer(
        transformers=[
            ('num', numeric_transformer, numeric_features),
            ('cat', categorical_transformer, categorical_features)
        ]
    )
    
    y = df.pop('rings')
    X_pre = preprocess.fit_transform(df)
    y_pre = y.to_numpy().reshape(len(y), 1)
    
    X = np.concatenate((y_pre, X_pre), axis=1)
    
    np.random.shuffle(X)
    # (80%, 15%, 5%) train/validation/test split
    training, validation, testing = np.split(X, [int(.8*len(X)), int(.95*len(X))])
    header = [
        'rings',
        'length',
        'diameter',
        'height',
        'whole_weight',
        'shucked_weight',
        'viscera_weight',
        'shell_weight',
        'sex_F',
        'sex_I',
        'sex_M'
    ]

    # Create the training and testing datasets for the SageMaker training Job
    pd.DataFrame(training).to_csv(f'{base_dir}/output/training/training.csv', header=False, index=False)
    pd.DataFrame(validation).to_csv(f'{base_dir}/output/training/validation.csv', header=False, index=False)
    pd.DataFrame(testing).to_csv(f'{base_dir}/output/testing/testing.csv', header=False, index=False)
    # pd.DataFrame(training).to_csv(f'{base_dir}/output/baseline/baseline.csv', header=header, index=False)
    
    # NOTE: For Model Quality Monitoring, the 'baseline' file is the predictions and labels. Creting the baseline
    #       will now happen in the 'evaluation step' for the workflow instead of the 'preprocessing step'
    # Take the testing data and use this as a Model Quality baseline (with headers)
    # pd.DataFrame(testing).to_csv(f'{base_dir}/output/baseline/baseline.csv', header=header, index=False)