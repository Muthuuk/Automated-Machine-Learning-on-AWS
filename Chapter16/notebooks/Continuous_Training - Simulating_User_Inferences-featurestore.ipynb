{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulating Continuous Training for the Production MLOps Pipeline\n",
    "\n",
    "## Overview\n",
    "\n",
    "__[BLAH BLAH BLAH: This step should only be run once the MLOps Pipeline has been deployed into production as it simulates what happens afterwards.]__\n",
    "\n",
    "## Section 1 - Setup\n",
    "\n",
    "><div class=\"alert alert-block alert-info\"><b>NOTE: </b>Recommend using an <em>ml.m5.large</em> (or larger) instance type and, <em>Python 3 (Data Science)</em> kernel to train the <b>CTGAN</b> model.</div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install CTGAN\n",
    "# !pip install CTGAN pyarrow==2 awswrangler==2.7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta, timezone\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import boto3\n",
    "import io\n",
    "import requests\n",
    "import tempfile\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "from time import sleep, gmtime, strftime\n",
    "from threading import Thread\n",
    "\n",
    "from sagemaker import get_execution_role, session, Session, image_uris\n",
    "from sagemaker.s3 import S3Downloader, S3Uploader\n",
    "from sagemaker.processing import ProcessingJob\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.model_monitor import DataCaptureConfig\n",
    "\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "boto_session = boto3.Session(region_name=region)\n",
    "sagemaker_client = boto_session.client(service_name='sagemaker', region_name=region)\n",
    "featurestore_runtime = boto_session.client(service_name='sagemaker-featurestore-runtime', region_name=region)\n",
    "session = Session()\n",
    "feature_store_session = Session(\n",
    "boto_session=boto_session,\n",
    "sagemaker_client=sagemaker_client,\n",
    "sagemaker_featurestore_runtime_client=featurestore_runtime\n",
    ")\n",
    "s3 = boto3.client('s3')\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Execution role\n",
    "role = get_execution_role()\n",
    "print(\"RoleArn:\", role)\n",
    "\n",
    "region = session.boto_region_name\n",
    "print(\"Region:\", region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline Data bucket\n",
    "data_bucket = 'data-us-east-2-500842391574'\n",
    "raw_key = 'input/raw/abalone.csv'\n",
    "print(f'Raw Data bucket: {data_bucket}')\n",
    "\n",
    "# Setup S3 bucket parmaters for the production logs bucket\n",
    "# Enter the name of the Production Logs Bucket, created by the MLOps Pipeline\n",
    "prod_bucket = 'proddeploymentstage-prodappl-logss3bucket004b0f70-qc1035xby1su'\n",
    "print(f'Production Logs Bucket: {prod_bucket}')\n",
    "\n",
    "# S3 prefixes\n",
    "data_capture_prefix = 'endpoint-data-capture'\n",
    "s3_capture_upload_path = f's3://{prod_bucket}/{data_capture_prefix}'\n",
    "ground_truth_upload_path = f's3://{prod_bucket}/ground-truth-data/{datetime.now():%Y-%m-%d-%H-%M-%S}'\n",
    "\n",
    "# Get the model monitor image\n",
    "monitor_image_uri = image_uris.retrieve(framework=\"model-monitor\", region=region)\n",
    "\n",
    "print(f'Image URI: {monitor_image_uri}')\n",
    "print(f'Capture path: {s3_capture_upload_path}')\n",
    "print(f'Ground truth path: {ground_truth_upload_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2 - Review Baseline Data\n",
    "\n",
    "We will re-create the Model Quality baseline job (even though it was already created by the CDK Pipeline) to see the output of the SageMaker SDK when calling `create_monitoring_schedule()`, as well as, to leverage the resultant constraints when creating the monitoring schedule. __[WRONG]__ The baseline suggestion is now part of the Production Model deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the locations for capturing the baseline results\n",
    "# This should already be in place from the CDK Pipeline,\n",
    "# with the `baseline.csv` already there\n",
    "baseline_prefix = 'baselining'\n",
    "baseline_data_prefix = baseline_prefix + '/data'\n",
    "baseline_results_prefix = baseline_prefix + '/results'\n",
    "\n",
    "baseline_dataset_uri = f's3://{prod_bucket}/{baseline_data_prefix}'\n",
    "baseline_results_uri = f's3://{prod_bucket}/{baseline_results_prefix}'\n",
    "print(f'Baseline data uri: {baseline_dataset_uri}')\n",
    "print(f'Baseline results uri: {baseline_results_uri}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the Generated Metrics\n",
    "\n",
    "__[Thes shoudl match what's in the Model Registry]__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics_obj = s3.get_object(Bucket=prod_bucket, Key=f'{baseline_results_prefix}/statistics.json')\n",
    "statistics_json = json.loads(statistics_obj['Body'].read().decode('utf-8'))['regression_metrics']\n",
    "pd.json_normalize(statistics_json).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the Generated Constraints\n",
    "\n",
    "__[Explain what these Constraints represent]__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constraints_obj = s3.get_object(Bucket=prod_bucket, Key=f'{baseline_results_prefix}/constraints.json')\n",
    "constraints_json = json.loads(constraints_obj['Body'].read().decode('utf-8'))['regression_constraints']\n",
    "pd.json_normalize(constraints_json).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3 - Create Inferene Data to Test the Model Quality Monitor\n",
    "\n",
    "Model Quality Monitoring needs two additional inputs - predictions made by the deployed model endpoint and the ground truth data to be provided by the model consuming application. Since you already enabled data capture on the endpoint, prediction data is captured in S3. The ground truth data depends on the what the model is predicting and what the business use case is.\n",
    "\n",
    "### Gerating Synthetic Abalone data\n",
    "\n",
    "In order to generate prediction data we will need to create fake \"new\" data. To accomplish this, we will use the CTGAN package and train it on the \"raw\" abaloen dataset. We will create $1000$ samples of fake data.\n",
    "\n",
    "__[Why $1000$ samples?]__\n",
    "\n",
    "\n",
    "><div class=\"alert alert-block alert-info\"><b>NOTE: </b>When adding <em>1000</em> samples to the 'raw' data without reshuffling, the model performance drastically underfits the <em>testing.csv</em> dataset. To create more data variance, the 'raw' data and 'feature store' data is shuffled.</div>\n",
    "\n",
    "__[Why CTGAN?]__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 'raw' data column names\n",
    "# names = [\n",
    "#     'sex',\n",
    "#     'length',\n",
    "#     'diameter',\n",
    "#     'height',\n",
    "#     'whole_weight',\n",
    "#     'shucked_weight',\n",
    "#     'viscera_weight',\n",
    "#     'shell_weight',\n",
    "#     'rings'\n",
    "# ]\n",
    "\n",
    "# # Location of the 'raw' data\n",
    "# obj = s3.get_object(Bucket=data_bucket, Key=raw_key)\n",
    "# raw_data = pd.read_csv(io.BytesIO(obj['Body'].read()), encoding='utf8', names=names)\n",
    "# raw_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">__NOTE:__ CTGAN training should take around 15 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ctgan import CTGANSynthesizer\n",
    "\n",
    "# # Fit the CTGAN model, declaring the 'sex' and 'rings' columns as discrete variables\n",
    "# ctgan = CTGANSynthesizer()\n",
    "# ctgan.fit(raw_data, ['sex', 'rings'], epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate 1000 samples from the CTGAN model\n",
    "# samples = ctgan.sample(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compare the raw data\n",
    "# raw_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compare the sample data\n",
    "# samples.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the samples as fake abalone data\n",
    "# samples.to_csv('fake-abalone.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing the Synthetic Abalone data for Continuous Model Training\n",
    "\n",
    "When the Model Quality Monitor (See Section 5) determines that model re-training is necessary, we will need \"new\" data to facilitate this. Along with using the Synthetic Abalone data to simulate user inferences, we will also make this data available to the MLOPs Pipeline for Continuos Training. Amazon SageMaker provides a fully managed, purpose-built repository to store the processed features in the form of the [Amazon SageMaker Feature Store](https://aws.amazon.com/sagemaker/feature-store/).\n",
    "\n",
    "><div class=\"alert alert-block alert-info\"><b>NOTE: </b>The following is code cell duplicates the functionality performed by the <em>preprocessing.py</em> script in the MLOps Pipeline.</div>\n",
    "\n",
    "__[Why are doing this?]__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.impute import SimpleImputer\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "\n",
    "# # Since we get a headerless CSV file we specify the column names here.\n",
    "# feature_columns_names = [\n",
    "#     'sex',\n",
    "#     'length',\n",
    "#     'diameter',\n",
    "#     'height',\n",
    "#     'whole_weight',\n",
    "#     'shucked_weight',\n",
    "#     'viscera_weight',\n",
    "#     'shell_weight',\n",
    "# ]\n",
    "# label_column = 'rings'\n",
    "\n",
    "# feature_columns_dtype = {\n",
    "#     'sex': str,\n",
    "#     'length': np.float64,\n",
    "#     'diameter': np.float64,\n",
    "#     'height': np.float64,\n",
    "#     'whole_weight': np.float64,\n",
    "#     'shucked_weight': np.float64,\n",
    "#     'viscera_weight': np.float64,\n",
    "#     'shell_weight': np.float64\n",
    "# }\n",
    "# label_column_dtype = {'rings': np.float64}\n",
    "\n",
    "\n",
    "# def merge_two_dicts(x, y):\n",
    "#     z = x.copy()\n",
    "#     z.update(y)\n",
    "#     return z\n",
    "\n",
    "\n",
    "# df = pd.read_csv(\n",
    "#     'fake-abalone.csv',\n",
    "#     header=None, \n",
    "#     names=feature_columns_names + [label_column],\n",
    "#     dtype=merge_two_dicts(feature_columns_dtype, label_column_dtype)\n",
    "# )\n",
    "\n",
    "# numeric_features = list(feature_columns_names)\n",
    "# numeric_features.remove('sex')\n",
    "# numeric_transformer = Pipeline(\n",
    "#     steps=[\n",
    "#         ('imputer', SimpleImputer(strategy='median')),\n",
    "#         ('scaler', StandardScaler())\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# categorical_features = ['sex']\n",
    "# categorical_transformer = Pipeline(\n",
    "#     steps=[\n",
    "#         ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "#         ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# preprocess = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         ('num', numeric_transformer, numeric_features),\n",
    "#         ('cat', categorical_transformer, categorical_features)\n",
    "#     ]\n",
    "# )\n",
    "    \n",
    "# y = df.pop('rings')\n",
    "# X_pre = preprocess.fit_transform(df)\n",
    "# y_pre = y.to_numpy().reshape(len(y), 1)\n",
    "# X = np.concatenate((y_pre, X_pre), axis=1)\n",
    "# np.random.shuffle(X)\n",
    "\n",
    "# new_header = [\n",
    "#     'rings',\n",
    "#     'length',\n",
    "#     'diameter',\n",
    "#     'height',\n",
    "#     'whole_weight',\n",
    "#     'shucked_weight',\n",
    "#     'viscera_weight',\n",
    "#     'shell_weight',\n",
    "#     'sex_F',\n",
    "#     'sex_I',\n",
    "#     'sex_M'\n",
    "# ]\n",
    "# new_training_data = pd.DataFrame(X, columns=new_header)\n",
    "\n",
    "# # View the the new, pre-processed training data\n",
    "# new_training_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the FeatureStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_time_sec = int(round(time.time()))\n",
    "\n",
    "# feature_group_name = 'AbaloneFeatureGroup'\n",
    "\n",
    "# abalone_feature_group = FeatureGroup(\n",
    "#     name=feature_group_name,\n",
    "#     sagemaker_session=feature_store_session\n",
    "# )\n",
    "\n",
    "# record_identifier_feature_name = 'rings'\n",
    "# event_time_feature_name = 'EventTime'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_training_data[event_time_feature_name] = pd.Series([current_time_sec]*len(new_training_data), dtype='float64')\n",
    "# new_training_data[event_time_feature_name] = pd.Series([current_time_sec]*len(new_training_data), dtype='float64')\n",
    "# new_training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abalone_feature_group.load_feature_definitions(\n",
    "#     data_frame=new_training_data\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def wait_for_feature_group_creation_complete(feature_group):\n",
    "#     status = feature_group.describe().get('FeatureGroupStatus')\n",
    "#     while status == 'Creating':\n",
    "#         print('Waiting for Feature Group Creation')\n",
    "#         time.sleep(5)\n",
    "#         status = feature_group.describe().get('FeatureGroupStatus')\n",
    "#     if status != 'Created':\n",
    "#         raise RuntimeError(f'Failed to create feature group {feature_group.name}')\n",
    "#     print(f'FeatureGroup {feature_group.name} successfully created.')\n",
    "\n",
    "# abalone_feature_group.create(\n",
    "#     s3_uri=f's3://{data_bucket}/featurestore',\n",
    "#     record_identifier_name=record_identifier_feature_name,\n",
    "#     event_time_feature_name=event_time_feature_name,\n",
    "#     role_arn=role,\n",
    "#     enable_online_store=True\n",
    "# )\n",
    "\n",
    "# wait_for_feature_group_creation_complete(feature_group=abalone_feature_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confirm FeatureStore Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abalone_feature_group.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingest Pre-Processed Synthetic Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abalone_feature_group.ingest(data_frame=new_training_data, max_workers=5, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confirm Data Ingest\n",
    "\n",
    "><div class=\"alert alert-block alert-info\"><b>NOTE: </b>Data ingestion should take around <em>6 - 7</em> Minutes.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_group_resolved_output_s3_uri = abalone_feature_group.describe().get(\"OfflineStoreConfig\").get(\"S3StorageConfig\").get(\"ResolvedOutputS3Uri\")\n",
    "# feature_group_s3_prefix = feature_group_resolved_output_s3_uri.replace(f\"s3://{data_bucket}/\", \"\")\n",
    "# offline_store_contents = None\n",
    "# while (offline_store_contents is None):\n",
    "#     objects_in_bucket = s3.list_objects(Bucket=data_bucket,Prefix=feature_group_s3_prefix)\n",
    "#     if ('Contents' in objects_in_bucket and len(objects_in_bucket['Contents']) > 1):\n",
    "#         offline_store_contents = objects_in_bucket['Contents']\n",
    "#     else:\n",
    "#         print('Waiting for data in offline store...\\n')\n",
    "#         sleep(60)\n",
    "\n",
    "# print('Data available.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create the Athena query instance\n",
    "# query = abalone_feature_group.athena_query()\n",
    "\n",
    "# # Get the name of the table to query\n",
    "# table = query.table_name\n",
    "\n",
    "# # Select the columns to get results\n",
    "# cols = [\n",
    "#     'rings',\n",
    "#     'length',\n",
    "#     'diameter',\n",
    "#     'height',\n",
    "#     'whole_weight',\n",
    "#     'shucked_weight',\n",
    "#     'viscera_weight',\n",
    "#     'shell_weight',\n",
    "#     'sex_F',\n",
    "#     'sex_I',\n",
    "#     'sex_M'\n",
    "# ]\n",
    "\n",
    "# # Create the SQL Query\n",
    "# query_string = f'SELECT {\",\".join(cols)} FROM \"{table}\"'\n",
    "\n",
    "# # Execute the query against Athena\n",
    "# query.run(query_string=query_string, output_location=f's3://{data_bucket}/query_results/')\n",
    "# query.wait()\n",
    "\n",
    "# # View Query results as a pandas DataFrame\n",
    "# results = query.as_dataframe()\n",
    "# results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4 - Setup Continuous Model Monitoring to identify model quality drift \n",
    "\n",
    "### Processing the Synthetic Abalone Data for User Inference Simulation\n",
    "\n",
    "Unlike the origional 'raw' dataset, we will not be performing any numerical pre-processing, as we need to create simualted raw data as user inference data. We only want to re-structure the inference data into a format that resembles the format used by the Website Form. This way, the inference requests will effectivley simulate users submit prediction requests using the Web Form. We will be using $300$ rndom samples.\n",
    "\n",
    "__[Why $300$ samples?]__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "\n",
    "# # Since we get a headerless CSV file we specify the column names here.\n",
    "# feature_columns_names = [\n",
    "#     'sex',\n",
    "#     'length',\n",
    "#     'diameter',\n",
    "#     'height',\n",
    "#     'whole_weight',\n",
    "#     'shucked_weight',\n",
    "#     'viscera_weight',\n",
    "#     'shell_weight',\n",
    "# ]\n",
    "# label_column = 'rings'\n",
    "\n",
    "# feature_columns_dtype = {\n",
    "#     'sex': str,\n",
    "#     'length': np.float64,\n",
    "#     'diameter': np.float64,\n",
    "#     'height': np.float64,\n",
    "#     'whole_weight': np.float64,\n",
    "#     'shucked_weight': np.float64,\n",
    "#     'viscera_weight': np.float64,\n",
    "#     'shell_weight': np.float64\n",
    "# }\n",
    "# label_column_dtype = {'rings': np.float64}\n",
    "\n",
    "\n",
    "# def merge_two_dicts(x, y):\n",
    "#     z = x.copy()\n",
    "#     z.update(y)\n",
    "#     return z\n",
    "\n",
    "\n",
    "# # Rstructure the data for inference for only 300 random samples\n",
    "# # n = 1000 # size of the dataset\n",
    "# # s = 200 # numer of tamples to take\n",
    "# # skip = sorted(random.sample(range(n), n-s))\n",
    "# df = pd.read_csv(\n",
    "#     'fake-abalone.csv',\n",
    "#     names=feature_columns_names + [label_column],\n",
    "#     dtype=merge_two_dicts(feature_columns_dtype, label_column_dtype),\n",
    "# #     skiprows=skip\n",
    "# )\n",
    "\n",
    "# # Separate the labels\n",
    "# y = df.pop('rings')\n",
    "\n",
    "# # Reorder colums to match inference format\n",
    "# cols = df.columns.values \n",
    "# reordered_cols = [\n",
    "#     'length',\n",
    "#     'diameter',\n",
    "#     'height',\n",
    "#     'whole_weight',\n",
    "#     'shucked_weight',\n",
    "#     'viscera_weight',\n",
    "#     'shell_weight',\n",
    "#     'sex'\n",
    "# ]\n",
    "# x = df.reindex(columns=reordered_cols)\n",
    "\n",
    "# # Create the inference dataset\n",
    "# x.to_csv('inference-data.csv', index=False)\n",
    "\n",
    "# # Create the ground truth dataset\n",
    "# y.to_csv('ground-truth.csv', header=['label'], index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should now have the following syntehtic data files:\n",
    "- `inference-data.csv`: Preprocessed data to generate predictions from the Production Endpoint.\n",
    "- `ground-truth.csv`: The ground truth labels from the synthetic data wich to compare the quality of the model's predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Inferences using the `FormProcessingAPI`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Endpoint name\n",
    "endpoint_name = 'abalone-prod-endpoint'\n",
    "\n",
    "# Form API Enpoint\n",
    "api_url = 'https://10r26vbp95.execute-api.us-east-2.amazonaws.com/'+'api/predict'\n",
    "\n",
    "def invoke_api(url, file_name):\n",
    "    df = pd.read_csv(file_name)\n",
    "    i = 0\n",
    "    for row in range(len(df)):\n",
    "        headers = {\"content-type\":\"application/json; charset=UTF-8\", \"inference-id\": str(i)}\n",
    "        body = json.loads(df.iloc[row].to_json())\n",
    "        response = requests.post(url, headers=headers, data=json.dumps(body))\n",
    "        i += 1\n",
    "        sleep(1)\n",
    "            \n",
    "def invoke_api_forever():\n",
    "    while True:\n",
    "        invoke_api(api_url, 'inference-data.csv')\n",
    "        \n",
    "api_thread = Thread(target=invoke_api_forever)\n",
    "api_thread.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View captured data stored in Amazon S3. You should expect to see different files from different time periods organized based on the hour in which the invocation occurred. The format of the Amazon S3 path is:\n",
    "\n",
    "`s3://{destination-bucket-prefix}/{endpoint-name}/{variant-name}/yyyy/mm/dd/hh/filename.jsonl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Waiting for captures to show up', end='')\n",
    "for _ in range(120): #2 Minutes\n",
    "    capture_files = sorted(S3Downloader.list(f'{s3_capture_upload_path}/{endpoint_name}'))\n",
    "    if capture_files:\n",
    "        capture_file = S3Downloader.read_file(capture_files[-1]).split('\\n')\n",
    "        capture_record = json.loads(capture_file[0])\n",
    "        if 'inferenceId' in capture_record['eventMetadata']:\n",
    "            break\n",
    "    print('.', end='', flush=True)\n",
    "    sleep(1)\n",
    "print()\n",
    "print('Found Capture Files:')\n",
    "print('\\n '.join(capture_files[-3:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the contents of a single capture file. Here you should see all the data captured in an Amazon SageMaker specific JSON-line formatted file. Take a quick peek at the first few lines in the captured file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n'.join(capture_file[-3:-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the contents of a single line is present below in a formatted JSON file so that you can observe a little better.\n",
    "\n",
    "><div class=\"alert alert-block alert-info\"><b>NOTE: </b>Take note of the <em>inferenceId</em>.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(capture_record, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Synthetic Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('ground-truth.csv')\n",
    "NUM_GROUND_TRUTH_RECORDS = len(df)\n",
    "\n",
    "def ground_truth_with_id(inference_id):\n",
    "    label = round(df.iloc[inference_id][0])\n",
    "    return {\n",
    "        'groundTruthData': {\n",
    "            'data': str(label),\n",
    "            'encoding': 'CSV'\n",
    "        },\n",
    "        'eventMetadata': {\n",
    "            'eventId': str(inference_id),\n",
    "        },\n",
    "        'eventVersion': '0',\n",
    "    }\n",
    "\n",
    "def upload_ground_truth(records, upload_time):\n",
    "    fake_records = [ json.dumps(r) for r in records ]\n",
    "    data_to_upload = '\\n'.join(fake_records)\n",
    "    target_s3_uri = f'{ground_truth_upload_path}/{upload_time:%Y/%m/%d/%H/%M%S}.jsonl'\n",
    "    print(f'Uploading {len(fake_records)} records to', target_s3_uri)\n",
    "    S3Uploader.upload_string_as_file_body(data_to_upload, target_s3_uri)\n",
    "\n",
    "def generate_fake_ground_truth_forever():\n",
    "    j = 0\n",
    "    while True:\n",
    "        fake_records = [ ground_truth_with_id(i) for i in range(NUM_GROUND_TRUTH_RECORDS) ]\n",
    "        upload_ground_truth(fake_records, datetime.utcnow())\n",
    "        j = (j + 1) % 5\n",
    "        sleep(60*60) # do this once an hour\n",
    "\n",
    "gt_thread = Thread(target=generate_fake_ground_truth_forever)\n",
    "gt_thread.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Appendix A: Manual process for Model Quality Monitoring\n",
    "\n",
    "## Quality Baselining\n",
    "\n",
    "### Manually Create Model Quality Baseline using the SageMaker SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a new baseline job\n",
    "from sagemaker.model_monitor import ModelQualityMonitor\n",
    "from sagemaker.model_monitor import EndpointInput\n",
    "from sagemaker.model_monitor.dataset_format import DatasetFormat\n",
    "\n",
    "# Create the model quality monitoring object\n",
    "model_quality_monitor = ModelQualityMonitor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    volume_size_in_gb=20,\n",
    "    max_runtime_in_seconds=1800,\n",
    "    sagemaker_session=session\n",
    ")\n",
    "\n",
    "# Name of the model quality baseline job\n",
    "baseline_job_name = f'abalone-baseline-{datetime.utcnow():%Y-%m-%d-%H%M}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually Suggest a Model Quality Baseline using the SageMaker SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the baseline suggestion job. \n",
    "# Specify problem type, in this case Regression, and provide other required attributes.\n",
    "job = model_quality_monitor.suggest_baseline(\n",
    "    job_name=baseline_job_name,\n",
    "    baseline_dataset=baseline_dataset_uri,\n",
    "    dataset_format=DatasetFormat.csv(header=True),\n",
    "    output_s3_uri = baseline_results_uri,\n",
    "    problem_type='Regression',\n",
    "    inference_attribute= \"prediction\",\n",
    "    ground_truth_attribute= \"label\"\n",
    ")\n",
    "job.wait(logs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_job = model_quality_monitor.latest_baselining_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_metrics = baseline_job.baseline_statistics().body_dict[\"regression_metrics\"]\n",
    "pd.json_normalize(regression_metrics).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(baseline_job.suggested_constraints().body_dict[\"regression_constraints\"]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_job.suggested_constraints().file_s3_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Quaility Monitoring Schedule\n",
    "\n",
    "### Manually Create a Model Quality Monitoring Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Endpoint name\n",
    "endpoint_name = 'abalone-prod-endpoint'\n",
    "\n",
    "# Monitoring schedule name\n",
    "monitor_schedule_name = f'abalone-monitoring-schedule-{datetime.utcnow():%Y-%m-%d-%H%M}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an enpointInput \n",
    "endpointInput = EndpointInput(endpoint_name=endpoint_name, \n",
    "                              inference_attribute='0',\n",
    "                              destination='/opt/ml/processing/input_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_monitor import CronExpressionGenerator\n",
    "\n",
    "response = model_quality_monitor.create_monitoring_schedule(\n",
    "    monitor_schedule_name=monitor_schedule_name,\n",
    "    endpoint_input=endpointInput,\n",
    "    output_s3_uri = baseline_results_uri,\n",
    "    problem_type='Regression',\n",
    "    ground_truth_input=ground_truth_upload_path,\n",
    "    constraints=f'{baseline_results_uri}/constraints.json',\n",
    "    schedule_cron_expression=CronExpressionGenerator.hourly(), \n",
    "    enable_cloudwatch_metrics=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the monitoring schedule\n",
    "#You will see the monitoring schedule in the 'Scheduled' status\n",
    "model_quality_monitor.describe_schedule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initially there will be no executions since the first execution happens at the top of the hour\n",
    "#Note that it is common for the execution to luanch upto 20 min after the hour.\n",
    "executions = model_quality_monitor.list_executions()\n",
    "executions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wait for the first execution of the Monitoring Schedule\n",
    "\n",
    ">__NOTE:__ This can take between $15$ and $20$ minutes past the top of the hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for the first execution of the monitoring_schedule\n",
    "print('Waiting for first execution', end='')\n",
    "while True:\n",
    "    execution = model_quality_monitor.describe_schedule().get('LastMonitoringExecutionSummary')\n",
    "    if execution:\n",
    "        break\n",
    "    print('.', end='', flush=True)\n",
    "    sleep(10)\n",
    "print()\n",
    "print('Execution found!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while not executions:\n",
    "    executions = model_quality_monitor.list_executions()\n",
    "    sleep(10)\n",
    "latest_execution = executions[-1]\n",
    "latest_execution.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review the Monitoring Schedule Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status = execution['MonitoringExecutionStatus']\n",
    "\n",
    "while status in ['Pending', 'InProgress']:\n",
    "    print('Waiting for execution to finish', end='')\n",
    "    latest_execution.wait(logs=False)\n",
    "    latest_job = latest_execution.describe()\n",
    "    print()\n",
    "    print(f\"{latest_job['ProcessingJobName']} job status:\", latest_job['ProcessingJobStatus'])\n",
    "    print(f\"{latest_job['ProcessingJobName']} job exit message, if any:\", latest_job.get('ExitMessage'))\n",
    "    print(f\"{latest_job['ProcessingJobName']} job failure reason, if any:\", latest_job.get('FailureReason'))\n",
    "    sleep(30) # model quality executions consist of two Processing jobs, wait for second job to start\n",
    "    latest_execution = model_quality_monitor.list_executions()[-1]\n",
    "    execution = model_quality_monitor.describe_schedule()['LastMonitoringExecutionSummary']\n",
    "    status = execution['MonitoringExecutionStatus']\n",
    "\n",
    "print(f'Execution status is: {status}')\n",
    "    \n",
    "if status != 'Completed':\n",
    "    print(execution)\n",
    "    print('====STOP==== \\n No completed executions to inspect further. Please wait till an execution completes or investigate previously reported failures.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_execution = model_quality_monitor.list_executions()[-1]\n",
    "report_uri = latest_execution.describe()['ProcessingOutputConfig']['Outputs'][0]['S3Output']['S3Uri']\n",
    "print(f'Report Uri: {report_uri}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = None\n",
    "violations = latest_execution.constraint_violations().body_dict['violations']\n",
    "violations_df = pd.json_normalize(violations)\n",
    "violations_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Quality CloudWatch Metrics\n",
    "\n",
    "### List the Generated CloudWatch Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create CloudWatch client\n",
    "# cw_client = boto3.Session().client('cloudwatch')\n",
    "\n",
    "# namespace = f'aws/sagemaker/Endpoints/model-metrics'\n",
    "\n",
    "# cw_dimensions=[\n",
    "#         {\n",
    "#             'Name': 'Endpoint',\n",
    "#             'Value': endpoint_name\n",
    "#         },\n",
    "#         {\n",
    "#             'Name': 'MonitoringSchedule',\n",
    "#             'Value': monitor_schedule_name\n",
    "#         }\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # List metrics through the pagination interface\n",
    "# paginator = cw_client.get_paginator('list_metrics')\n",
    "\n",
    "# for response in paginator.paginate(Dimensions=cw_dimensions,Namespace=namespace):\n",
    "#     model_quality_metrics = response['Metrics']\n",
    "#     for metric in model_quality_metrics:\n",
    "#         print(metric['MetricName'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the CloudWatch Alarm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CloudWatch client\n",
    "cw_client = boto3.Session().client('cloudwatch')\n",
    "\n",
    "alarm_name='MODEL_QUALITY_ALARM'\n",
    "alarm_desc='Trigger an CloudWatch alarm when the rmse score drifts away from the baseline constraints'\n",
    "\n",
    "# Setting the threshold to match the rmse threshold from the baseline evaluation\n",
    "model_quality_rmse_threshold=3.1\n",
    "metric_name='rmse'\n",
    "namespace = f'aws/sagemaker/Endpoints/model-metrics'\n",
    "\n",
    "cw_client.put_metric_alarm(\n",
    "    AlarmName=alarm_name,\n",
    "    AlarmDescription=alarm_desc,\n",
    "    ActionsEnabled=True,\n",
    "    MetricName=metric_name,\n",
    "    Namespace=namespace,\n",
    "    Statistic='Average',\n",
    "    Dimensions=[\n",
    "        {\n",
    "            'Name': 'Endpoint',\n",
    "            'Value': endpoint_name\n",
    "        },\n",
    "        {\n",
    "            'Name': 'MonitoringSchedule',\n",
    "            'Value': monitor_schedule_name\n",
    "        }\n",
    "    ],\n",
    "    Period=5400,\n",
    "    EvaluationPeriods=1,\n",
    "    DatapointsToAlarm=1,\n",
    "    Threshold=model_quality_rmse_threshold,\n",
    "    ComparisonOperator='GreaterThanThreshold',\n",
    "    TreatMissingData='missing'\n",
    "#     TreatMissingData='breaching'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_quality_monitor.delete_monitoring_schedule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.m5.large",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
