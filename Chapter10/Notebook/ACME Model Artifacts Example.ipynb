{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACME Model ArtifactsExample\n",
    "\n",
    ">__NOTE:__ This Notebook uses the _Python 3 (Data Science)_ Kernel.\n",
    "\n",
    "## Configuring the Model Artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile model/model.py\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import re\n",
    "import traceback\n",
    "import pathlib\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "tf.get_logger().setLevel(\"ERROR\")\n",
    "\n",
    "\n",
    "prefix = \"/opt/ml\"\n",
    "processing_path = os.path.join(prefix, \"processing\")\n",
    "preprocessing_input_path = os.path.join(processing_path, \"input/data\")\n",
    "preprocessing_output_path = os.path.join(processing_path, \"output\")\n",
    "training_input_path = os.path.join(prefix, \"input/data\")\n",
    "evaluation_input_path = os.path.join(processing_path, \"input\")\n",
    "evaluation_output_path = os.path.join(processing_path, \"output/evaluation\")\n",
    "output_path = os.path.join(prefix, \"output\")\n",
    "model_path = os.path.join(prefix, \"model\")\n",
    "param_path = os.path.join(prefix, \"input/config/hyperparameters.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a model/model.py\n",
    "\n",
    "\n",
    "def train():\n",
    "    print(\"Training mode\")\n",
    "    try:\n",
    "        channel_name = \"training\"\n",
    "        training_path = os.path.join(training_input_path, channel_name)\n",
    "        params = {}\n",
    "        with open(param_path, \"r\") as f:\n",
    "            is_float = re.compile(r'^\\d+(?:\\.\\d+)$')\n",
    "            is_integer = re.compile(r'^\\d+$')\n",
    "            for key,value in json.load(f).items():\n",
    "                if is_float.match(value) is not None:\n",
    "                    value = float(value)\n",
    "                elif is_integer.match(value) is not None:\n",
    "                    value = int(value)\n",
    "                params[key] = value\n",
    "\n",
    "        input_files = [ os.path.join(training_path, file) for file in os.listdir(training_path) ]\n",
    "        if len(input_files) == 0:\n",
    "            raise ValueError((f\"There are no files in {training_path}.\\\\n\" +\n",
    "                              f\"This usually indicates that the channel ({channel_name}) was incorrectly specified,\\\\n\" +\n",
    "                              \"the data specification in S3 was incorrectly specified or the role specified\\\\n\" +\n",
    "                              \"does not have permission to access the data.\"))\n",
    "        column_names = [\"rings\", \"length\", \"diameter\", \"height\", \"whole weight\", \"shucked_weight\", \"viscera_weight\", \"shell_weight\", \"sex_F\", \"sex_I\", \"sex_M\"]\n",
    "        train_data = pd.read_csv(os.path.join(training_path, \"training.csv\"), sep=',', names=column_names)\n",
    "        val_data = pd.read_csv(os.path.join(training_path, \"validation.csv\"), sep=',', names=column_names)\n",
    "        train_y = train_data[\"rings\"].to_numpy()\n",
    "        train_X = train_data.drop([\"rings\"], axis=1).to_numpy()\n",
    "        val_y = val_data[\"rings\"].to_numpy()\n",
    "        val_X = val_data.drop([\"rings\"], axis=1).to_numpy()\n",
    "        train_X = preprocessing.normalize(train_X)\n",
    "        val_X = preprocessing.normalize(val_X)\n",
    "        network_layers = [\n",
    "            Dense(64, activation=\"relu\", kernel_initializer=\"normal\", input_dim=10),\n",
    "            Dense(64, activation=\"relu\"),\n",
    "            Dense(1, activation=\"linear\")\n",
    "        ]\n",
    "        model = Sequential(network_layers)\n",
    "        model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\", \"accuracy\"])\n",
    "        model.summary()\n",
    "        model.fit(train_X, train_y, validation_data=(val_X, val_y),\n",
    "                  batch_size=params.get(\"batch_size\"), epochs=params.get(\"epochs\"),\n",
    "                  shuffle=True, verbose=1\n",
    "        )\n",
    "        print(\"Saving Model\")\n",
    "        model.save(filepath=os.path.join(model_path, \"model.h5\"), overwrite=True, include_optimizer=False, save_format=\"h5\")\n",
    "\n",
    "    except Exception as e:\n",
    "        trc = traceback.format_exc()\n",
    "        with open(os.path.join(output_path, \"failure\"), \"w\") as f:\n",
    "            f.write(\"Exception during training: {}\".format(str(e) + '\\\\n' + trc))\n",
    "        print(\"Exception during training: {}\".format(str(e) + '\\\\n' + trc), file=sys.stderr)\n",
    "        sys.exit(255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Create the Application\n",
    "\n",
    "### Container entrypoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile model/app.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import json\n",
    "import io\n",
    "import sys\n",
    "import os\n",
    "import signal\n",
    "import traceback\n",
    "import flask\n",
    "import multiprocessing\n",
    "import subprocess\n",
    "import tarfile\n",
    "import model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "prefix = \"/opt/ml\"\n",
    "model_path = os.path.join(prefix, \"model\")\n",
    "sys.path.insert(0,model_path)\n",
    "model_cache = {}\n",
    "\n",
    "class PredictionService(object):\n",
    "    tf_model = None\n",
    "    @classmethod\n",
    "    def get_model(cls):\n",
    "        if cls.tf_model is None:\n",
    "            cls.tf_model = load_model()\n",
    "        return cls.tf_model\n",
    "\n",
    "    @classmethod\n",
    "    def predict(cls, input):\n",
    "        tf_model = cls.get_model()\n",
    "        return tf_model.predict(input)\n",
    "\n",
    "def load_model():\n",
    "    model = tf.keras.models.load_model(os.path.join(model_path, \"model.h5\"))\n",
    "    model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "    return model\n",
    "\n",
    "def sigterm_handler(nginx_pid, gunicorn_pid):\n",
    "    try:\n",
    "        os.kill(nginx_pid, signal.SIGQUIT)\n",
    "    except OSError:\n",
    "        pass\n",
    "    try:\n",
    "        os.kill(gunicorn_pid, signal.SIGTERM)\n",
    "    except OSError:\n",
    "        pass\n",
    "\n",
    "    sys.exit(0)\n",
    "\n",
    "def start_server(timeout, workers):\n",
    "    print(f\"Starting the inference server with {model_server_workers} workers\")\n",
    "    subprocess.check_call([\"ln\", \"-sf\", \"/dev/stdout\", \"/var/log/nginx/access.log\"])\n",
    "    subprocess.check_call([\"ln\", \"-sf\", \"/dev/stderr\", \"/var/log/nginx/error.log\"])\n",
    "    nginx = subprocess.Popen([\"nginx\", \"-c\", \"/opt/program/nginx.conf\"])\n",
    "    gunicorn = subprocess.Popen([\"gunicorn\",\n",
    "                                 \"--timeout\", str(timeout),\n",
    "                                 \"-k\", \"gevent\",\n",
    "                                 \"-b\", \"unix:/tmp/gunicorn.sock\",\n",
    "                                 \"-w\", str(workers),\n",
    "                                 \"wsgi:app\"])\n",
    "\n",
    "    signal.signal(signal.SIGTERM, lambda a, b: sigterm_handler(nginx.pid, gunicorn.pid))\n",
    "    pids = set([nginx.pid, gunicorn.pid])\n",
    "    while True:\n",
    "        pid, _ = os.wait()\n",
    "        if pid in pids:\n",
    "            break\n",
    "    sigterm_handler(nginx.pid, gunicorn.pid)\n",
    "    print(\"Inference server exiting\")\n",
    "\n",
    "\n",
    "app = flask.Flask(__name__)\n",
    "\n",
    "\n",
    "@app.route(\"/ping\", methods=[\"GET\"])\n",
    "def ping():\n",
    "    health = PredictionService.get_model() is not None\n",
    "    status = 200 if health else 404\n",
    "    return flask.Response(response=\"\\n\", status=status, mimetype=\"application/json\")\n",
    "\n",
    "\n",
    "@app.route(\"/invocations\", methods=[\"POST\"])\n",
    "def invoke():\n",
    "    data = None\n",
    "    if flask.request.content_type == \"text/csv\":\n",
    "        payload = np.fromstring(flask.request.data.decode('utf-8'), sep=\",\")\n",
    "        data = payload.reshape(1, -1)\n",
    "    else:\n",
    "        return flask.Response(response=\"Invalid request data type, only 'text/csv' is supported.\", status=415, mimetype=\"text/plain\")\n",
    "    predictions = PredictionService.predict(data)\n",
    "    out = io.StringIO()\n",
    "    pd.DataFrame({\"results\": predictions.flatten()}).to_csv(out, header=False, index=False)\n",
    "    result = out.getvalue()\n",
    "    print(f\"Prediction Result: {result}\")\n",
    "    return flask.Response(response=result, status=200, mimetype=\"text/csv\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"Tensorflow Version: {tf.__version__}\")\n",
    "    if len(sys.argv) < 2 or ( not sys.argv[1] in [ \"serve\", \"train\"] ):\n",
    "        raise Exception(\"Invalid argument: you must specify 'train' for training mode, 'serve' for predicting mode, 'preprocess' for preprocessing mode or 'evaluate' for evaluation mode.\") \n",
    "    train = sys.argv[1] == \"train\"\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        cpu_count = multiprocessing.cpu_count()\n",
    "        model_server_timeout = os.environ.get('MODEL_SERVER_TIMEOUT', 60)\n",
    "        model_server_workers = int(os.environ.get('MODEL_SERVER_WORKERS', cpu_count))\n",
    "        start_server(model_server_timeout, model_server_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nginx Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile model/nginx.conf\n",
    "worker_processes 1;\n",
    "daemon off;\n",
    "\n",
    "pid /tmp/nginx.pid;\n",
    "error_log /var/log/nginx/error.log;\n",
    "\n",
    "events {\n",
    "\n",
    "}\n",
    "\n",
    "http {\n",
    "  include /etc/nginx/mime.types;\n",
    "  default_type application/octet-stream;\n",
    "  access_log /var/log/nginx/access.log combined;\n",
    "  \n",
    "  upstream gunicorn {\n",
    "    server unix:/tmp/gunicorn.sock;\n",
    "  }\n",
    "\n",
    "  server {\n",
    "\n",
    "    listen 8080 deferred;\n",
    "    client_max_body_size 5m;\n",
    "\n",
    "    keepalive_timeout 5;\n",
    "\n",
    "    location ~ ^/(ping|invocations) {\n",
    "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
    "      proxy_set_header Host $http_host;\n",
    "      proxy_redirect off;\n",
    "      proxy_pass http://gunicorn;\n",
    "    }\n",
    "\n",
    "    location / {\n",
    "      return 404 \"{}\";\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Server Gateay Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile model/wsgi.py\n",
    "import app as myapp\n",
    "app = myapp.app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Create the Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile model/Dockerfile\n",
    "ARG REGION\n",
    "FROM 763104351884.dkr.ecr.${REGION}.amazonaws.com/tensorflow-training:2.6.0-cpu-py38-ubuntu20.04\n",
    "RUN apt-get update && apt-get install -y --no-install-recommends \\\n",
    "    nginx &&\\\n",
    "    rm -rf /var/lib/apt/lists/*\n",
    "RUN pip install --upgrade pip\n",
    "RUN pip install --no-cache-dir --upgrade \\\n",
    "    flask \\\n",
    "    gevent \\\n",
    "    gunicorn \\\n",
    "    pyarrow==2 \\\n",
    "    awswrangler\n",
    "RUN mkdir -p /opt/program\n",
    "RUN mkdir -p /opt/ml\n",
    "COPY app.py /opt/program\n",
    "COPY model.py /opt/program\n",
    "COPY nginx.conf /opt/program\n",
    "COPY wsgi.py /opt/program\n",
    "WORKDIR /opt/program\n",
    "EXPOSE 8080\n",
    "ENTRYPOINT [\"python\", \"app.py\"]"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
